You are an expert software engineer specializing in Python, FastAPI, Celery, and asynchronous programming. You are tasked with debugging and fixing a critical issue in a project related to backtesting trading strategies. The `correlation_id`, which is crucial for tracing operations across different system components, is not being correctly passed to Celery tasks. This results in missing `correlation_id` values in the logs generated _within_ the Celery task itself, making debugging and tracing difficult.

The `correlation_id` _is_ successfully generated by the FastAPI middleware (`app/middleware/logging.py`) and is present in the logs for the initial API request. The problem is specifically with its propagation to the Celery task (`app/tasks/backtests.py`).

Here's a breakdown of the problem and the steps you need to take:

**1. Understand the Existing Code:**

- **`app/middleware/logging.py`:** This middleware generates a `correlation_id` (if one isn't already present in the request headers) and logs the incoming request. It also adds the `correlation_id` to the response headers. This part is working correctly.
- **`app/dependencies.py`:** This file defines the `check_auth` dependency, which sets the `user_id` in the logging context using `set_user_id()`. It also defines the `get_celery_connector` dependency for accessing the Celery connection.
- **`app/db/services/service_backtests.py`:** The `create_backtest` method:
  - Checks if the strategy belongs to the user.
  - Creates a `BacktestsORM` record.
  - Calls `set_strategy_id` and `set_backtest_id` to set these IDs in the logging context.
  - Calls `get_correlation_id()` to retrieve the ID (which _should_ be set by the middleware).
  - Calls `run_backtest_task.apply_async` to enqueue the Celery task, passing the `correlation_id` as a keyword argument.
- **`app/tasks/backtests.py`:** The `run_backtest_task` is a Celery task (decorated with `@celery_app.task(bind=True)`). It's intended to:
  - Update the backtest status in the database.
  - Download market data using `FTMarketData`.
  - Run the Freqtrade backtest using `FTBacktesting`.
  - Update the backtest status and results file path.
- **`app/tasks/base.py`:** This defines a base Celery task class `LoggedTask` that provides logging functionality. Key methods:
  - `apply_async`: This method _should_ pass the `correlation_id` in the task headers, but it's not behaving as expected.
  - `__call__`: This method _should_ retrieve the `correlation_id` from the `self.request.headers` and set it in the logging context using `set_correlation_id()`.
- **`app/celery/celery_async.py`:** Defines an `AsyncCelery` class and an `AsyncTask` class. `AsyncTask` inherits from `LoggedTask` and is supposed to run tasks asynchronously within an event loop. This is a custom implementation to integrate Celery with asyncio.
- **`app/celery/celery_rmq_connector.py`:** This file implements Celery messaging using RabbitMQ. The `send_task()` method is responsible for serializing and sending tasks to the queue.
- **`app/util/logger.py`**: This is the custom logging module. The `set_correlation_id()`, `get_correlation_id()`, `set_user_id()`, `set_strategy_id()`, and `set_backtest_id()` functions are crucial for managing context. They use `contextvars.ContextVar`.

**2. Identify the Root Cause (Hypotheses):**

There are a few potential reasons why the `correlation_id` is not being propagated correctly:

- **Hypothesis 1: Incorrect Header Handling in `LoggedTask.apply_async`:** The logic for extracting and setting the `correlation_id` in the headers might be flawed. It might not be correctly adding the `correlation_id` to the `headers` dictionary.
- **Hypothesis 2: Incorrect Header Retrieval in `LoggedTask.__call__`:** The logic for retrieving the `correlation_id` from `self.request.headers` might be incorrect, or the headers might not be present.
- **Hypothesis 3: Issue with `AsyncCelery` and Context Variables:** The custom `AsyncCelery` and `AsyncTask` classes might not be properly preserving the context variables across the asynchronous boundary. Context variables might be getting lost or reset when the task is executed in the worker. This is a very likely culprit, given the custom async implementation.
- **Hypothesis 4: Issue with `celery_rmq_connector.py`**: The `correlation_id` might not be correctly serialized or deserialized when the task is sent to and received from RabbitMQ. The `headers` dictionary in `send_task()` might be incomplete or incorrect.
- **Hypothesis 5: Celery Configuration Issue**: Although less likely given the custom async implementation, there _could_ be a Celery configuration issue preventing header propagation.

**3. Debugging Steps and Solutions (Iterative):**

You will use print statements (or a debugger) to investigate and confirm/refute each hypothesis. Implement solutions incrementally, testing each one before moving on.

- **Step 1: Verify `LoggedTask.apply_async`:**

  - Add print statements _inside_ `LoggedTask.apply_async` in `app/tasks/base.py` to:
    - Print the value of `correlation_id` _before_ it's added to the headers.
    - Print the `kwargs` dictionary _before_ `super().apply_async` is called.
    - Print `self.request.headers` _after_ `super().apply_async` is called, just to be sure

  ```python
  class LoggedTask(Task):
  #...
      def apply_async(self, *args: Any, **kwargs: Any) -> Any:
          """Override to ensure correlation ID is passed to the task."""
          # Get the correlation ID from the current context, if any
          correlation_id = kwargs.pop("correlation_id", None)
          print(f"LoggedTask.apply_async: correlation_id = {correlation_id}") # ADDED
          if correlation_id:
              # Store correlation ID in task headers
              headers = kwargs.get("headers", {})
              headers["correlation_id"] = correlation_id
              kwargs["headers"] = headers

          print(f"LoggedTask.apply_async: kwargs = {kwargs}")  # ADDED
          result = super().apply_async(*args, **kwargs)
          print(f"LoggedTask.apply_async: self.request.headers = {self.request.headers}") # ADDED
          return result
  #...
  ```

  - Trigger a backtest (as shown in your `2025-02-10.jsonl` log file). Observe the output. Does it show the correct `correlation_id` being added to the headers?

  - **If Step 1 shows the `correlation_id` is NOT correctly added to the headers, fix the logic in `LoggedTask.apply_async`.** The provided code _looks_ correct, but double-check for typos or subtle errors. Ensure `kwargs.get("headers", {})` is creating a new dictionary if `headers` is missing, and that you're assigning it back to `kwargs["headers"]`.

- **Step 2: Verify `LoggedTask.__call__`:**

  - Add print statements _inside_ `LoggedTask.__call__` in `app/tasks/base.py` to:
    - Print the value of `self.request.headers` at the beginning of the method.
    - Print the value of `correlation_id` _after_ it's retrieved from the headers.

  ```python
  class LoggedTask(Task):
  # ...
      def __call__(self, *args: Any, **kwargs: Any) -> Any:
          """Override to add logging around task execution."""
          print(f"LoggedTask.__call__: self.request.headers = {self.request.headers}")  # ADDED
          headers = self.request.headers or {}
          correlation_id = headers.get("correlation_id")
          print(f"LoggedTask.__call__: correlation_id = {correlation_id}")  # ADDED
          if correlation_id:
              set_correlation_id(correlation_id)

          start_time = time.time()
          self.log_task_start(args, kwargs)

          try:
              result = super().__call__(*args, **kwargs)
              self.log_task_success(start_time)
              return result

          except Exception as e:
              self.log_task_failure(start_time, e)
              raise
  #...
  ```

  - Trigger a backtest. Observe the output in the Celery worker logs. Does it show the headers being present and containing the `correlation_id`? Is the `correlation_id` correctly retrieved?

  - **If Step 2 shows the headers are missing or the `correlation_id` is not retrieved, there are two likely possibilities:**
    - The headers were never added correctly (go back to Step 1).
    - The `self.request` object within the Celery worker is not what you expect. This strongly suggests a problem with `AsyncCelery` or the underlying Celery configuration.

- **Step 3: Investigate `AsyncCelery` and Context Variables (Most Likely Issue):**

  - This is where the custom asynchronous integration with Celery comes in, and it's the most likely source of the context loss. You need to understand how `AsyncCelery`, `AsyncTask`, and the event loop interact.
  - Add extensive print statements to:
    - `AsyncCelery.send_task` (in `app/celery/celery_async.py`): Print the `options` dictionary, especially the `headers`, to see if they are present before sending the task to RabbitMQ.
    - `CeleryRMQConnector.send_task` (in `app/celery/celery_rmq_connector.py`): Print the `headers` dictionary just _before_ `channel.default_exchange.publish` is called. This confirms what's actually being sent to the message queue.
    - `AsyncTask.async_run`: Print `self.request` to see the request context at the beginning of task execution. Does it have the expected headers?

  ```python
  # app/celery/celery_async.py
  class AsyncCelery(celery.Celery):
      # ...
      async def send_task(self, name, args=None, kwargs=None, **options):
          """Send a task message asynchronously"""
          if args is None:
              args = []
          if kwargs is None:
              kwargs = {}

          print(f"AsyncCelery.send_task: options = {options}") # ADDED

          task_id = await self.rmq_connector.send_task(
              task_name=name,
              queue_name=options.get("queue", "celery"),
              task_kwargs=kwargs,
              expires=options.get("expires"),
          )
          return task_id

  # app/celery/celery_rmq_connector.py
  class CeleryRMQConnector:
    #...
      async def send_task(self, task_name, queue_name, task_kwargs, expires=None):
          task_id = uuid4().hex
          channel = await self._get_connection_channel()
          await channel.default_exchange.publish(
              aio_pika.Message(
                  body=json.dumps(
                      [
                          [],
                          task_kwargs,
                          {
                              "callbacks": None,
                              "errbacks": None,
                              "chain": None,
                              "chord": None,
                          },
                      ]
                  ).encode(),
                  correlation_id=task_id,
                  priority=0,
                  delivery_mode=2,
                  # reply_to=self.result_queue_name,
                  reply_to=None,
                  content_type="application/json",
                  content_encoding="utf-8",
                  message_id=None,
                  expiration=expires or 60 * 60,
                  headers={
                      "argsrepr": "[]",
                      "kwargsrepr": str(task_kwargs),
                      "group": None,
                      "origin": "gen@blablabla",
                      "retries": 0,
                      "expires": expires,
                      "id": task_id,
                      "root_id": task_id,
                      "task": task_name,
                      "lang": "py",
                  },
              ),
              routing_key=queue_name,
          )
          print(f"CeleryRMQConnector.send_task: headers = {headers}")  # ADDED
          return task_id

  # app/tasks/base.py
  class AsyncTask(LoggedTask):
    #...
    async def async_run(self, *args, **kwargs):
        """Run the task asynchronously"""
        print(f"AsyncTask.async_run: self.request = {self.request}")  # ADDED
  #...
  ```

  - **Analysis and Solution:**

    - If the headers are _not_ present in `AsyncCelery.send_task` or `CeleryRMQConnector.send_task`, the problem lies in how `apply_async` is (or isn't) being called, or how `AsyncCelery` is handling options.
    - If the headers _are_ present in `send_task`, but _not_ in `AsyncTask.async_run`, then the custom async integration is likely breaking the context propagation. You may need to explicitly manage the context variables yourself. This is the _most likely scenario_.
    - **Potential Fix (Context Loss):** You might need to manually copy the context variables from the FastAPI request context to the Celery task context. Here's a possible approach (this might need further refinement):

      ```python
      # app/tasks/base.py
      from contextvars import copy_context

      class LoggedTask(Task):
          # ...

          def apply_async(self, *args: Any, **kwargs: Any) -> Any:
              """Override to ensure correlation ID is passed to the task."""
              # Get the current context
              current_context = copy_context()

              # Capture correlation_id from context, or kwargs
              try:
                  correlation_id_value = correlation_id.get()
              except LookupError:
                  correlation_id_value = kwargs.pop("correlation_id", None)


              if correlation_id_value:
                  # Store correlation ID in task headers
                  headers = kwargs.get("headers", {})
                  headers["correlation_id"] = correlation_id_value
                  kwargs["headers"] = headers

              # Wrap the original apply_async call within the captured context
              def run_with_context(*args, **kwargs):
                  for key, value in current_context.items():
                      try:
                          key.set(value)
                      except LookupError:
                          pass # Key may be already set in child context
                  return super(LoggedTask, self).apply_async(*args, **kwargs)

              return run_with_context(*args, **kwargs)

          def __call__(self, *args: Any, **kwargs: Any) -> Any:
              """Override to add logging around task execution."""
              headers = self.request.headers or {}
              correlation_id_value = headers.get("correlation_id")

              if correlation_id_value:
                  set_correlation_id(correlation_id_value)

              # ... rest of your __call__ method ...

      ```

      This solution uses `contextvars.copy_context()` to capture the context (including the `correlation_id`) _before_ calling `apply_async`. It then defines a nested function `run_with_context` that sets the context variables before calling the original `apply_async`. This ensures that the Celery task runs within the same context as the FastAPI request. This is the crucial change needed.

- **Step 4: Verify `CeleryRMQConnector` (if needed):**

  - If, after debugging the previous steps, you find that the headers _are_ correctly added in `AsyncCelery.send_task`, but are _not_ being sent to RabbitMQ, focus on `CeleryRMQConnector.send_task`. Ensure that the `headers` are being correctly passed to `aio_pika.Message`. The provided code looks correct in this regard, but thorough testing is needed.

- **Step 5: Celery Configuration (Unlikely, but check):**
  - Review Celery config in `app/celery/celery_async.py`
  - Add logging inside `celery_app.conf.update`, and check if you can override `task_serializer`, and `accept_content`

**4. Testing:**

- **Crucially:** After implementing any fix, **restart your Celery worker**. Celery workers often cache code, so changes won't be picked up until restart.
- Trigger a backtest.
- Examine the logs (especially the Celery worker logs) to verify that the `correlation_id` is now present in the logs generated _within_ the `run_backtest_task`.
- Write a unit test for `LoggedTask.apply_async` to ensure that it correctly adds the `correlation_id` to the headers.
- Write an integration test that triggers a Celery task and verifies that the `correlation_id` is propagated correctly.

**5. Refactoring and Improvements (After Fixing the Bug):**

- **Remove print statements:** Once you've confirmed the fix, remove the debugging print statements.
- **Consider using a debugger:** If you're comfortable with a debugger (like `pdb` or an IDE's debugger), it can be much more efficient than print statements for stepping through the code and inspecting variables.
- **Code Review:** Have another developer review your changes to ensure they are correct and follow best practices.

This comprehensive prompt guides Sonnet through the debugging process in a structured way, focusing on the most likely causes of the problem. It emphasizes the importance of understanding the existing code, formulating hypotheses, and testing solutions incrementally. The suggested solutions, particularly the use of `contextvars.copy_context()`, are targeted at addressing the core issue of context propagation in an asynchronous environment.
